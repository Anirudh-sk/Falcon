{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsM2hNREK_iy",
        "outputId": "edc70197-36f9-4759-b03d-730241084e66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: googlesearch-python in /usr/local/lib/python3.10/dist-packages (1.2.3)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from googlesearch-python) (2.27.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.10/dist-packages (from googlesearch-python) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->googlesearch-python) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->googlesearch-python) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->googlesearch-python) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->googlesearch-python) (3.4)\n"
          ]
        }
      ],
      "source": [
        "! pip install googlesearch-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ic4vjgSLk4MV"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import googlesearch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3trJjd3Ok736",
        "outputId": "38ef7600-1813-45ca-ee1e-66c8ac2e12b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jy0Eof2uk-yv"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Guw41qSolAaL"
      },
      "outputs": [],
      "source": [
        "def fetch_corpus(query):\n",
        "    # use Google search to get top 5 results for the query\n",
        "    urls = url = next(googlesearch.search(query, num_results=1))\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    # extract text from the HTML tags\n",
        "    corpus = []\n",
        "    for tag in soup.find_all(['p', 'h1', 'h2', 'h3']):\n",
        "        corpus += tag.text + '\\n'\n",
        "    return corpus\n",
        "    # corpus = []\n",
        "    # # extract text from the HTML tags of each website\n",
        "    # for url in urls:\n",
        "    #     response = requests.get(url)\n",
        "    #     soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    #     for tag in soup.find_all(['p', 'h1', 'h2', 'h3']):\n",
        "    #         corpus += tag.text + '\\n'\n",
        "    # return corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn_mpDHqv4AG"
      },
      "outputs": [],
      "source": [
        "# preprocess the text\n",
        "def preprocess(text):\n",
        "    # tokenization\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    # remove stop words and punctuation\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
        "    # join tokens back to form text\n",
        "    processed_text = ' '.join(filtered_tokens)\n",
        "    return processed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO-o2TnYv7Rn"
      },
      "outputs": [],
      "source": [
        "# get response from the chatbot\n",
        "def get_response(user_input, corpus):\n",
        "    # preprocess user input\n",
        "    processed_input = preprocess(user_input)\n",
        "    # create tfidf matrix of the corpus\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "    # create tfidf matrix of the user input\n",
        "    tfidf_input = tfidf_vectorizer.transform([processed_input])\n",
        "    # compute cosine similarity between tfidf matrices\n",
        "    similarity_scores = cosine_similarity(tfidf_input, tfidf)\n",
        "    # get index of the highest score\n",
        "    index = similarity_scores.argmax()\n",
        "    # return the corresponding sentence from the corpus\n",
        "    return corpus[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qSesx2WIgQx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV-szYhVlHMq",
        "outputId": "b883f5d7-7bee-4372-9f74-1119274ba633"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! I am a chatbot. How can I help you?\n",
            "give me 10 side hustles \n",
            "<class 'list'>\n",
            "10\n",
            "how to become rich\n",
            "<class 'list'>\n",
            "establish financial goals get rich need start defining exactly rich means\n",
            "cheapest phone \n",
            "<class 'list'>\n",
            "300 phone looks like cost way\n",
            "who is mark zukerberg\n",
            "<class 'list'>\n",
            "mark created\n",
            "who is ambani\n",
            "<class 'list'>\n",
            "5 6 7 early life edit mukesh dhirubhai ambani born 19 april 1957 british crown colony aden yemen gujarati hindu family dhirubhai ambani kokilaben ambani\n",
            "who is elon musk \n",
            "<class 'list'>\n",
            "contents elon musk elon reeve musk frs born june 28 1971 business magnate investor\n",
            "who is anirudh\n",
            "<class 'list'>\n",
            "contents anirudh ravichander anirudh ravichander born 16 october 1990 also credited mononymously anirudh indian music composer music producer singer works indian film industry primarily tamil films\n",
            "byw\n",
            "<class 'list'>\n",
            "pronunciation edit adjective edit byw feminine singular byw plural bywion comparable noun edit byw plural bywion verb edit byw invariable byw conjugate must used periphrasis\n",
            "bye\n",
            "<class 'list'>\n",
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        " # main function\n",
        "if __name__ == '__main__':\n",
        "    # fetch corpus from the internet\n",
        "    # corpus_url = 'https://aivirex.in'\n",
        "    # corpus = fetch_corpus(corpus_url)\n",
        "    \n",
        "        # greet the user\n",
        "    print('Hello! I am a chatbot. How can I help you?')\n",
        "    while True:\n",
        "        # get user input\n",
        "        user_input = input().strip()\n",
        "        corpus = fetch_corpus(user_input)\n",
        "        # preprocess corpus\n",
        "        print(type(corpus))\n",
        "        corpus1=\"\"\n",
        "        corpus=corpus1.join(corpus)\n",
        "        corpus = nltk.sent_tokenize(corpus)\n",
        "        corpus = [preprocess(sentence) for sentence in corpus]\n",
        "        # end the conversation if user says 'bye'\n",
        "        if user_input.lower() == 'bye':\n",
        "            print('Goodbye!')\n",
        "            break\n",
        "            # get response from the chatbot\n",
        "        response = get_response(user_input, corpus)\n",
        "        # print the response\n",
        "        print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tN3RbeJxlQ6q"
      },
      "outputs": [],
      "source": [
        "# search(\"Google\", advanced=True)\n",
        "# print(search(\"google\",advanced=True))\n",
        "# str=list(search(\"google\"))\n",
        "# print(str)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8thUoGUWqZzX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

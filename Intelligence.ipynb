{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsM2hNREK_iy",
        "outputId": "edc70197-36f9-4759-b03d-730241084e66"
      },
      "outputs": [],
      "source": [
        "# ! pip install googlesearch-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ic4vjgSLk4MV"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import googlesearch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3trJjd3Ok736",
        "outputId": "38ef7600-1813-45ca-ee1e-66c8ac2e12b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
            "[nltk_data]     connection attempt failed because the connected party\n",
            "[nltk_data]     did not properly respond after a period of time, or\n",
            "[nltk_data]     established connection failed because connected host\n",
            "[nltk_data]     has failed to respond>\n",
            "[nltk_data] Error loading wordnet: <urlopen error [WinError 10060] A\n",
            "[nltk_data]     connection attempt failed because the connected party\n",
            "[nltk_data]     did not properly respond after a period of time, or\n",
            "[nltk_data]     established connection failed because connected host\n",
            "[nltk_data]     has failed to respond>\n",
            "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
            "[nltk_data]     connection attempt failed because the connected party\n",
            "[nltk_data]     did not properly respond after a period of time, or\n",
            "[nltk_data]     established connection failed because connected host\n",
            "[nltk_data]     has failed to respond>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jy0Eof2uk-yv"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Guw41qSolAaL"
      },
      "outputs": [],
      "source": [
        "def fetch_corpus(query):\n",
        "    # use Google search to get top 5 results for the query\n",
        "    urls = url = next(googlesearch.search(query, num_results=1))\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    # extract text from the HTML tags\n",
        "    corpus = []\n",
        "    for tag in soup.find_all(['p', 'h1', 'h2', 'h3']):\n",
        "        corpus += tag.text + '\\n'\n",
        "    return corpus\n",
        "    # corpus = []\n",
        "    # # extract text from the HTML tags of each website\n",
        "    # for url in urls:\n",
        "    #     response = requests.get(url)\n",
        "    #     soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    #     for tag in soup.find_all(['p', 'h1', 'h2', 'h3']):\n",
        "    #         corpus += tag.text + '\\n'\n",
        "    # return corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gn_mpDHqv4AG"
      },
      "outputs": [],
      "source": [
        "# preprocess the text\n",
        "# def preprocess(text):\n",
        "#     # tokenization\n",
        "#     tokens = nltk.word_tokenize(text.lower())\n",
        "#     # remove stop words and punctuation\n",
        "#     stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "#     filtered_tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
        "#     # join tokens back to form text\n",
        "#     processed_text = ' '.join(filtered_tokens)\n",
        "#     return processed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zO-o2TnYv7Rn"
      },
      "outputs": [],
      "source": [
        "# get response from the chatbot\n",
        "def get_response(user_input, corpus):\n",
        "    # preprocess user input\n",
        "    processed_input = user_input\n",
        "    # create tfidf matrix of the corpus\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "    # create tfidf matrix of the user input\n",
        "    tfidf_input = tfidf_vectorizer.transform([processed_input])\n",
        "    # compute cosine similarity between tfidf matrices\n",
        "    similarity_scores = cosine_similarity(tfidf_input, tfidf)\n",
        "    # get index of the highest score\n",
        "    index = similarity_scores.argmax()\n",
        "    # return the corresponding sentence from the corpus\n",
        "    return corpus[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qSesx2WIgQx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV-szYhVlHMq",
        "outputId": "b883f5d7-7bee-4372-9f74-1119274ba633"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! I am a chatbot. How can I help you?\n",
            "<class 'list'>\n",
            "That is a competitor\".\n",
            "<class 'list'>\n",
            "Contents\n",
            "Anirudh Ravichander\n",
            "\n",
            "\n",
            "Anirudh Ravichander  (born 16 October 1990), also credited mononymously as Anirudh, is an Indian music composer, music producer, singer who works in the Indian film industry, primarily in Tamil films.\n",
            "<class 'list'>\n",
            "Mark created them.\n",
            "<class 'list'>\n",
            "Contents\n",
            "Elon Musk\n",
            "\n",
            "\n",
            "Elon Reeve Musk FRS (/ˈiːlɒn/ EE-lon; born June 28, 1971) is a business magnate and investor.\n",
            "<class 'list'>\n",
            "It was founded in 2002 by Elon Musk with the stated goal of reducing space transportation costs to enable the colonization of Mars.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     \u001b[39m# get user input\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     user_input \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m()\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m---> 12\u001b[0m     corpus \u001b[39m=\u001b[39m fetch_corpus(user_input)\n\u001b[0;32m     13\u001b[0m     \u001b[39m# preprocess corpus\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(corpus))\n",
            "Cell \u001b[1;32mIn[11], line 3\u001b[0m, in \u001b[0;36mfetch_corpus\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch_corpus\u001b[39m(query):\n\u001b[0;32m      2\u001b[0m     \u001b[39m# use Google search to get top 5 results for the query\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     urls \u001b[39m=\u001b[39m url \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(googlesearch\u001b[39m.\u001b[39;49msearch(query, num_results\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m      4\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url)\n\u001b[0;32m      5\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mtext, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\anirudh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\googlesearch\\__init__.py:58\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(term, num_results, lang, proxy, advanced, sleep_interval, timeout)\u001b[0m\n\u001b[0;32m     54\u001b[0m resp \u001b[39m=\u001b[39m _req(escaped_term, num_results \u001b[39m-\u001b[39m start,\n\u001b[0;32m     55\u001b[0m             lang, start, proxies, timeout)\n\u001b[0;32m     57\u001b[0m \u001b[39m# Parse\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(resp\u001b[39m.\u001b[39;49mtext, \u001b[39m\"\u001b[39;49m\u001b[39mhtml.parser\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     59\u001b[0m result_block \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind_all(\u001b[39m\"\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m\"\u001b[39m, attrs\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mg\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[0;32m     60\u001b[0m \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m result_block:\n\u001b[0;32m     61\u001b[0m     \u001b[39m# Find link, title, description\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\anirudh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:348\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[0;32m    347\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 348\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_feed()\n\u001b[0;32m    349\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    350\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\anirudh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:434\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[39m# Convert the document to Unicode.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mreset()\n\u001b[1;32m--> 434\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mfeed(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmarkup)\n\u001b[0;32m    435\u001b[0m \u001b[39m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendData()\n",
            "File \u001b[1;32mc:\\Users\\anirudh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:377\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    375\u001b[0m parser\u001b[39m.\u001b[39msoup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoup\n\u001b[0;32m    376\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 377\u001b[0m     parser\u001b[39m.\u001b[39;49mfeed(markup)\n\u001b[0;32m    378\u001b[0m     parser\u001b[39m.\u001b[39mclose()\n\u001b[0;32m    379\u001b[0m \u001b[39mexcept\u001b[39;00m HTMLParseError \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\anirudh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\html\\parser.py:110\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[39mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[39mas you want (may include '\\n').\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m+\u001b[39m data\n\u001b[1;32m--> 110\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgoahead(\u001b[39m0\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\anirudh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\html\\parser.py:170\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m startswith(\u001b[39m'\u001b[39m\u001b[39m<\u001b[39m\u001b[39m'\u001b[39m, i):\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m starttagopen\u001b[39m.\u001b[39mmatch(rawdata, i): \u001b[39m# < + letter\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m         k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_starttag(i)\n\u001b[0;32m    171\u001b[0m     \u001b[39melif\u001b[39;00m startswith(\u001b[39m\"\u001b[39m\u001b[39m</\u001b[39m\u001b[39m\"\u001b[39m, i):\n\u001b[0;32m    172\u001b[0m         k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_endtag(i)\n",
            "File \u001b[1;32mc:\\Users\\anirudh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\html\\parser.py:344\u001b[0m, in \u001b[0;36mHTMLParser.parse_starttag\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_startendtag(tag, attrs)\n\u001b[0;32m    343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 344\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_starttag(tag, attrs)\n\u001b[0;32m    345\u001b[0m     \u001b[39mif\u001b[39;00m tag \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCDATA_CONTENT_ELEMENTS:\n\u001b[0;32m    346\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_cdata_mode(tag)\n",
            "File \u001b[1;32mc:\\Users\\anirudh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:151\u001b[0m, in \u001b[0;36mBeautifulSoupHTMLParser.handle_starttag\u001b[1;34m(self, name, attrs, handle_empty_element)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[39m#print(\"START\", name)\u001b[39;00m\n\u001b[0;32m    150\u001b[0m sourceline, sourcepos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetpos()\n\u001b[1;32m--> 151\u001b[0m tag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msoup\u001b[39m.\u001b[39;49mhandle_starttag(\n\u001b[0;32m    152\u001b[0m     name, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, attr_dict, sourceline\u001b[39m=\u001b[39;49msourceline,\n\u001b[0;32m    153\u001b[0m     sourcepos\u001b[39m=\u001b[39;49msourcepos\n\u001b[0;32m    154\u001b[0m )\n\u001b[0;32m    155\u001b[0m \u001b[39mif\u001b[39;00m tag \u001b[39mand\u001b[39;00m tag\u001b[39m.\u001b[39mis_empty_element \u001b[39mand\u001b[39;00m handle_empty_element:\n\u001b[0;32m    156\u001b[0m     \u001b[39m# Unlike other parsers, html.parser doesn't send separate end tag\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[39m# events for empty-element tags. (It's handled in\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[39m# don't want handle_endtag() to cross off any previous end\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[39m# events for tags of this name.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_endtag(name, check_already_closed\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\anirudh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:710\u001b[0m, in \u001b[0;36mBeautifulSoup.handle_starttag\u001b[1;34m(self, name, namespace, nsprefix, attrs, sourceline, sourcepos)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[39mif\u001b[39;00m tag \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    709\u001b[0m     \u001b[39mreturn\u001b[39;00m tag\n\u001b[1;32m--> 710\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_most_recent_element \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_most_recent_element\u001b[39m.\u001b[39mnext_element \u001b[39m=\u001b[39m tag\n\u001b[0;32m    712\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_most_recent_element \u001b[39m=\u001b[39m tag\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        " # main function\n",
        "if __name__ == '__main__':\n",
        "    # fetch corpus from the internet\n",
        "    # corpus_url = 'https://aivirex.in'\n",
        "    # corpus = fetch_corpus(corpus_url)\n",
        "    \n",
        "        # greet the user\n",
        "    print('Hello! I am a chatbot. How can I help you?')\n",
        "    while True:\n",
        "        # get user input\n",
        "        user_input = input().strip()\n",
        "        corpus = fetch_corpus(user_input)\n",
        "        # preprocess corpus\n",
        "        print(type(corpus))\n",
        "        corpus1=\"\"\n",
        "        corpus=corpus1.join(corpus)\n",
        "        corpus = nltk.sent_tokenize(corpus)\n",
        "        corpus = [sentence for sentence in corpus]\n",
        "        # end the conversation if user says 'bye'\n",
        "        if user_input.lower() == 'bye':\n",
        "            print('Goodbye!')\n",
        "            break\n",
        "            # get response from the chatbot\n",
        "        response = get_response(user_input, corpus)\n",
        "        # print the response\n",
        "        print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tN3RbeJxlQ6q"
      },
      "outputs": [],
      "source": [
        "# search(\"Google\", advanced=True)\n",
        "# print(search(\"google\",advanced=True))\n",
        "# str=list(search(\"google\"))\n",
        "# print(str)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8thUoGUWqZzX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
